{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch \n",
    "from utils import plot_learning_curve\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch \n",
    "from torch import nn, optim, utils, device as device_, cuda\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Deep Q network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDQNetwork(nn.Module): #linear deep q netwrok\n",
    "    def __init__(self, lr, n_actions, input_dims):\n",
    "        super(LinearDQNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(*input_dims, 128) #the fist fully connected layer takes in the inputs and retirns 128 layers\n",
    "        self.fc2 = nn.Linear(128, n_actions) #the second fully connected layer takes in the 128 layers as \n",
    "        #inputs and returns value of actions (estimate for Q)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cup') #query torch device if gpu not avaliable results in cpu\n",
    "        self.to(self.device) # send all network to device\n",
    "        \n",
    "        \n",
    "    def forwardpass(self, state): #forward pass the states and return actions\n",
    "        layer1 = F.relu(self.fc1(state))\n",
    "        actions = self.fc2(layer1)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, input_dims, n_actions, lr, gamma=0.99,\n",
    "                epsilon=1.0, eps_dec=1e-5, eps_min=0.01):\n",
    "\n",
    "        self.lr = lr\n",
    "        self.input_dims = input_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_dec = eps_dec\n",
    "        self.eps_min = eps_min\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "\n",
    "        self.Q = LinearDQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "        \n",
    "        \n",
    "#         def choose_action(self, observation):\n",
    "#             if np.rnadom.random() > self.epsilon: #random value for greedy action, if greater than epsilion take a random action\n",
    "                \n",
    "#                 state = torch.tensor(observation, dtype=torch.float).to(self.Q.device) #obsertaion to torch tensor and send to device\n",
    "#                 actions = self.Q.forwardpass(state) #value of actions for a given state\n",
    "#                 action = torch.argmax(actions).item() \n",
    "#             else:\n",
    "#                 actions = np.random.choice(self.action_space)\n",
    "#                 return action\n",
    "        \n",
    "        def choose_action(self, observation):\n",
    "            if np.random.random() > self.epsilon:\n",
    "                state  = T.tensor(observation, dtype=T.float).to(self.Q.device)\n",
    "                actions = self.Q.forward(state)\n",
    "                action = T.argmax(actions).item()\n",
    "            else:\n",
    "                action = np.random.choice(self.action_space)\n",
    "\n",
    "            \n",
    "#             #find the max of each action - derefrence by .iteam()\n",
    "#             #since torch.argmax(actions) retusn torhc tensors and we need numpy arrays which is required for open ai gym\n",
    "#         def decreament_epsilon(self):\n",
    "#             self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "    \n",
    "#     # decrase epsilion linarly - it will reach minimum (finite) over time\n",
    "    \n",
    "#       #how the agent will learn from its experince - current action state reward and new state as inputs\n",
    "\n",
    "#         def learn(self, state, actions, reward, state_new):\n",
    "#             self.Q.optimizer.zero_grad() #zero gradients\n",
    "#             state = torch.tensors(state, dtype=torch.float).to(self.Q.device) #arrays to cuda tensors\n",
    "#             actions = torch.tensors(action).to(self.Q.device)\n",
    "#             rewards = torch.tensor(reward).to(self.Q.device)\n",
    "#             states_new = torch.tensor(state_new, dtype=torch.float).to(self.Q.device) #all numpy arrays as torch tensors\n",
    "\n",
    "\n",
    "#             #feed forward to update Q estimate\n",
    "#             Q_estimate = self.Q.forward(states)[actions] #predicted values for currnt state of the environment \n",
    "#             #the change between the action taken and total possible actions\n",
    "            \n",
    "#             #the maximum actions of the resulting state\n",
    "#             Q_estimate_next = self.Q.forward(state_next).max() \n",
    "            \n",
    "#             #direction to move \n",
    "#             Q_target = reward + self.gamma*Q_estimate_next\n",
    "            \n",
    "#             #distance between the action taken and the maximum action the agent could take (mean squred error)\n",
    "#             loss = self.Q.loss(Q_target, Q_estimate).to(self.Q.device)\n",
    "            \n",
    "#             #backpropagte \n",
    "#             loss.backward.setp()\n",
    "#             #setp optimizer\n",
    "#             self.Q.optimizer.step()\n",
    "#             self.decreament_epsilon()\n",
    "\n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "                if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    def learn(self, state, action, reward, state_):\n",
    "        self.Q.optimizer.zero_grad()\n",
    "        states = T.tensor(state, dtype=T.float).to(self.Q.device)\n",
    "        actions = T.tensor(action).to(self.Q.device)\n",
    "        rewards = T.tensor(reward).to(self.Q.device)\n",
    "        states_ = T.tensor(state_, dtype=T.float).to(self.Q.device)\n",
    "\n",
    "        q_pred = self.Q.forward(states)[actions]\n",
    "\n",
    "        q_next = self.Q.forward(states_).max()\n",
    "\n",
    "        q_target = reward + self.gamma*q_next\n",
    "\n",
    "        loss = self.Q.loss(q_target, q_pred).to(self.Q.device)\n",
    "        loss.backward()\n",
    "        self.Q.optimizer.step()\n",
    "        self.decrement_epsilon()\n",
    "        \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 score 17.0 average score 17.0 epsilon 1.00\n",
      "episode 100 score 42.0 average score 21.5 epsilon 0.98\n",
      "episode 200 score 35.0 average score 21.9 epsilon 0.96\n",
      "episode 300 score 52.0 average score 20.2 epsilon 0.94\n",
      "episode 400 score 12.0 average score 24.3 epsilon 0.91\n",
      "episode 500 score 12.0 average score 21.8 epsilon 0.89\n",
      "episode 600 score 24.0 average score 22.3 epsilon 0.87\n",
      "episode 700 score 15.0 average score 22.6 epsilon 0.85\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    n_games = 10000\n",
    "    scores = []\n",
    "    eps_history = []\n",
    "\n",
    "    agent = Agent(input_dims=env.observation_space.shape, \n",
    "                n_actions=env.action_space.n, lr=0.0001)\n",
    "\n",
    "    for i in range(n_games):\n",
    "        score = 0\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(obs)\n",
    "            obs_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.learn(obs, action, reward, obs_)\n",
    "            obs = obs_\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print(\"episode\", i, \"score %.1f average score %.1f epsilon %.2f\" %\n",
    "               (score, avg_score, agent.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 score 31.0 average score 31.0 epsilon 1.00\n",
      "episode 100 score 22.0 average score 21.7 epsilon 0.98\n",
      "episode 200 score 28.0 average score 19.7 epsilon 0.96\n",
      "episode 300 score 14.0 average score 21.3 epsilon 0.94\n",
      "episode 400 score 18.0 average score 21.2 epsilon 0.92\n",
      "episode 500 score 11.0 average score 23.9 epsilon 0.89\n",
      "episode 600 score 22.0 average score 19.7 epsilon 0.87\n",
      "episode 700 score 14.0 average score 22.6 epsilon 0.85\n",
      "episode 800 score 21.0 average score 22.0 epsilon 0.83\n",
      "episode 900 score 26.0 average score 23.7 epsilon 0.80\n",
      "episode 1000 score 20.0 average score 26.1 epsilon 0.78\n",
      "episode 1100 score 22.0 average score 24.5 epsilon 0.75\n",
      "episode 1200 score 10.0 average score 24.2 epsilon 0.73\n",
      "episode 1300 score 11.0 average score 23.6 epsilon 0.71\n",
      "episode 1400 score 18.0 average score 28.0 epsilon 0.68\n",
      "episode 1500 score 44.0 average score 29.0 epsilon 0.65\n",
      "episode 1600 score 90.0 average score 23.8 epsilon 0.62\n",
      "episode 1700 score 15.0 average score 27.1 epsilon 0.60\n",
      "episode 1800 score 11.0 average score 22.3 epsilon 0.58\n",
      "episode 1900 score 19.0 average score 33.7 epsilon 0.54\n",
      "episode 2000 score 12.0 average score 31.8 epsilon 0.51\n",
      "episode 2100 score 10.0 average score 36.1 epsilon 0.47\n",
      "episode 2200 score 15.0 average score 31.7 epsilon 0.44\n",
      "episode 2300 score 10.0 average score 30.5 epsilon 0.41\n",
      "episode 2400 score 13.0 average score 32.8 epsilon 0.38\n",
      "episode 2500 score 64.0 average score 32.4 epsilon 0.35\n",
      "episode 2600 score 9.0 average score 35.3 epsilon 0.31\n",
      "episode 2700 score 12.0 average score 44.2 epsilon 0.27\n",
      "episode 2800 score 10.0 average score 44.4 epsilon 0.22\n",
      "episode 2900 score 9.0 average score 37.2 epsilon 0.18\n",
      "episode 3000 score 27.0 average score 53.5 epsilon 0.13\n",
      "episode 3100 score 19.0 average score 41.9 epsilon 0.09\n",
      "episode 3200 score 20.0 average score 53.8 epsilon 0.04\n",
      "episode 3300 score 51.0 average score 43.1 epsilon 0.01\n",
      "episode 3400 score 46.0 average score 49.3 epsilon 0.01\n",
      "episode 3500 score 10.0 average score 16.9 epsilon 0.01\n",
      "episode 3600 score 9.0 average score 9.4 epsilon 0.01\n",
      "episode 3700 score 47.0 average score 39.1 epsilon 0.01\n",
      "episode 3800 score 42.0 average score 47.6 epsilon 0.01\n",
      "episode 3900 score 26.0 average score 34.6 epsilon 0.01\n",
      "episode 4000 score 28.0 average score 28.8 epsilon 0.01\n",
      "episode 4100 score 20.0 average score 25.9 epsilon 0.01\n",
      "episode 4200 score 20.0 average score 23.0 epsilon 0.01\n",
      "episode 4300 score 27.0 average score 23.1 epsilon 0.01\n",
      "episode 4400 score 22.0 average score 22.8 epsilon 0.01\n",
      "episode 4500 score 22.0 average score 25.9 epsilon 0.01\n",
      "episode 4600 score 20.0 average score 22.3 epsilon 0.01\n",
      "episode 4700 score 13.0 average score 16.1 epsilon 0.01\n",
      "episode 4800 score 19.0 average score 14.2 epsilon 0.01\n",
      "episode 4900 score 40.0 average score 38.8 epsilon 0.01\n",
      "episode 5000 score 60.0 average score 47.1 epsilon 0.01\n",
      "episode 5100 score 37.0 average score 42.2 epsilon 0.01\n",
      "episode 5200 score 28.0 average score 42.1 epsilon 0.01\n",
      "episode 5300 score 50.0 average score 47.2 epsilon 0.01\n",
      "episode 5400 score 83.0 average score 54.6 epsilon 0.01\n",
      "episode 5500 score 61.0 average score 62.6 epsilon 0.01\n",
      "episode 5600 score 58.0 average score 49.2 epsilon 0.01\n",
      "episode 5700 score 45.0 average score 59.4 epsilon 0.01\n",
      "episode 5800 score 44.0 average score 61.6 epsilon 0.01\n",
      "episode 5900 score 164.0 average score 66.0 epsilon 0.01\n",
      "episode 6000 score 14.0 average score 425.4 epsilon 0.01\n",
      "episode 6100 score 21.0 average score 28.2 epsilon 0.01\n",
      "episode 6200 score 20.0 average score 17.4 epsilon 0.01\n",
      "episode 6300 score 15.0 average score 16.3 epsilon 0.01\n",
      "episode 6400 score 14.0 average score 16.5 epsilon 0.01\n",
      "episode 6500 score 16.0 average score 16.4 epsilon 0.01\n",
      "episode 6600 score 16.0 average score 16.7 epsilon 0.01\n",
      "episode 6700 score 16.0 average score 16.5 epsilon 0.01\n",
      "episode 6800 score 16.0 average score 16.5 epsilon 0.01\n",
      "episode 6900 score 10.0 average score 14.0 epsilon 0.01\n",
      "episode 7000 score 21.0 average score 14.6 epsilon 0.01\n",
      "episode 7100 score 500.0 average score 251.4 epsilon 0.01\n",
      "episode 7200 score 170.0 average score 421.7 epsilon 0.01\n",
      "episode 7300 score 500.0 average score 437.4 epsilon 0.01\n",
      "episode 7400 score 9.0 average score 305.3 epsilon 0.01\n",
      "episode 7500 score 9.0 average score 9.4 epsilon 0.01\n",
      "episode 7600 score 9.0 average score 9.6 epsilon 0.01\n",
      "episode 7700 score 10.0 average score 9.9 epsilon 0.01\n",
      "episode 7800 score 8.0 average score 10.1 epsilon 0.01\n",
      "episode 7900 score 10.0 average score 11.8 epsilon 0.01\n",
      "episode 8000 score 9.0 average score 9.4 epsilon 0.01\n",
      "episode 8100 score 78.0 average score 70.7 epsilon 0.01\n",
      "episode 8200 score 23.0 average score 53.4 epsilon 0.01\n",
      "episode 8300 score 19.0 average score 19.6 epsilon 0.01\n",
      "episode 8400 score 16.0 average score 18.2 epsilon 0.01\n",
      "episode 8500 score 23.0 average score 19.6 epsilon 0.01\n",
      "episode 8600 score 18.0 average score 21.7 epsilon 0.01\n",
      "episode 8700 score 22.0 average score 21.2 epsilon 0.01\n",
      "episode 8800 score 32.0 average score 22.2 epsilon 0.01\n",
      "episode 8900 score 29.0 average score 26.8 epsilon 0.01\n",
      "episode 9000 score 33.0 average score 29.3 epsilon 0.01\n",
      "episode 9100 score 26.0 average score 31.8 epsilon 0.01\n",
      "episode 9200 score 41.0 average score 40.5 epsilon 0.01\n",
      "episode 9300 score 52.0 average score 57.1 epsilon 0.01\n",
      "episode 9400 score 67.0 average score 67.1 epsilon 0.01\n",
      "episode 9500 score 82.0 average score 74.7 epsilon 0.01\n",
      "episode 9600 score 58.0 average score 47.8 epsilon 0.01\n",
      "episode 9700 score 51.0 average score 62.2 epsilon 0.01\n",
      "episode 9800 score 500.0 average score 135.1 epsilon 0.01\n",
      "episode 9900 score 211.0 average score 336.6 epsilon 0.01\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "\n",
    "class LinearDeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, n_actions, input_dims):\n",
    "        super(LinearDeepQNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(*input_dims, 128)\n",
    "        self.fc2 = nn.Linear(128, n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss  = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        layer1 = F.relu(self.fc1(state))\n",
    "        action = self.fc2(layer1)\n",
    "\n",
    "        return action\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, input_dims, n_actions, lr, gamma=0.99,\n",
    "                epsilon=1.0, eps_dec=1e-5, eps_min=0.01):\n",
    "\n",
    "        self.lr = lr\n",
    "        self.input_dims = input_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_dec = eps_dec\n",
    "        self.eps_min = eps_min\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "\n",
    "        self.Q = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state  = T.tensor(observation, dtype=T.float).to(self.Q.device)\n",
    "            actions = self.Q.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "                if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    def learn(self, state, action, reward, state_):\n",
    "        self.Q.optimizer.zero_grad()\n",
    "        states = T.tensor(state, dtype=T.float).to(self.Q.device)\n",
    "        actions = T.tensor(action).to(self.Q.device)\n",
    "        rewards = T.tensor(reward).to(self.Q.device)\n",
    "        states_ = T.tensor(state_, dtype=T.float).to(self.Q.device)\n",
    "\n",
    "        q_pred = self.Q.forward(states)[actions]\n",
    "\n",
    "        q_next = self.Q.forward(states_).max()\n",
    "\n",
    "        q_target = reward + self.gamma*q_next\n",
    "\n",
    "        loss = self.Q.loss(q_target, q_pred).to(self.Q.device)\n",
    "        loss.backward()\n",
    "        self.Q.optimizer.step()\n",
    "        self.decrement_epsilon()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    n_games = 10000\n",
    "    scores = []\n",
    "    eps_history = []\n",
    "\n",
    "    agent = Agent(input_dims=env.observation_space.shape, \n",
    "                n_actions=env.action_space.n, lr=0.0001)\n",
    "\n",
    "    for i in range(n_games):\n",
    "        score = 0\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(obs)\n",
    "            obs_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.learn(obs, action, reward, obs_)\n",
    "            obs = obs_\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print(\"episode\", i, \"score %.1f average score %.1f epsilon %.2f\" %\n",
    "               (score, avg_score, agent.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
